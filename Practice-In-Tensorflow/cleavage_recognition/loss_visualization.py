#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2019/5/29 12:58
# @Author  : Yajun Yin
# @Note    :


import matplotlib.pyplot as plt
import numpy as np

# lr = 0.001 step=15000 batch=256 opt=sgd
# test ac = 0.919 prec= 0.925 recall = 0.9085
val_loss = [0.68701243, 0.5449513, 0.47120553, 0.44631162, 0.43098167, 0.38142616, 0.40780428, 0.42196727, 0.3728982,
            0.38438004, 0.38372844, 0.36133373, 0.36216503, 0.35396206, 0.31880388, 0.34463406, 0.36151248, 0.32069135,
            0.31436253, 0.3438431, 0.25514162, 0.29943666, 0.27995718, 0.3100465, 0.30712178, 0.30065462, 0.28719175,
            0.30073375, 0.3162539, 0.29556578, 0.28921175, 0.31091273, 0.31582403, 0.27969494, 0.29488456, 0.31415036,
            0.30155504, 0.3198142, 0.27010465, 0.27974117, 0.31848812, 0.3046302, 0.298675, 0.29244804, 0.27609044,
            0.29341817, 0.28702188, 0.27122656, 0.30959153, 0.28756875, 0.29027748, 0.2611423, 0.24512863, 0.32231855,
            0.27610534, 0.3338542, 0.27450293, 0.31370068, 0.27588534, 0.2564869, 0.31524712, 0.30240113, 0.24665141,
            0.26307225, 0.27852243, 0.2541459, 0.2639591, 0.26259392, 0.25371975, 0.28432596, 0.27435765, 0.27467483,
            0.28760353, 0.29211003, 0.2662614, 0.24909896, 0.23640949, 0.31747383, 0.28278834, 0.2774451, 0.25334406,
            0.2542973, 0.2641875, 0.26260293, 0.3174172, 0.26565844, 0.2617886, 0.28273305, 0.23713645, 0.25179097,
            0.27221447, 0.28343892, 0.28332245, 0.28592253, 0.28162754, 0.2324568, 0.24810529, 0.2637345, 0.27261293,
            0.23756768, 0.2649878, 0.24214458, 0.26700467, 0.23478071, 0.2744236, 0.26071048, 0.23461314, 0.24602458,
            0.26551557, 0.22937429, 0.27220055, 0.2771433, 0.25691128, 0.26921737, 0.25438756, 0.26769084, 0.25112164,
            0.22964948, 0.25271088, 0.24538568, 0.24580759, 0.26904273, 0.29057953, 0.2505236, 0.26872152, 0.28274754,
            0.27091295, 0.24019971, 0.19637942, 0.23457196, 0.2564868, 0.2529979, 0.23266585, 0.213364, 0.20858517,
            0.25169274, 0.23620331, 0.27072424, 0.22479162, 0.25707108, 0.2505167, 0.28327402, 0.25414833, 0.21220136,
            0.20723633, 0.24754015, 0.21870254, 0.2227486, 0.23255791, 0.28155458, 0.2585593, 0.24555756, 0.22238356,
            0.25224444, 0.25843877, 0.2377321, 0.21776578, 0.26291892, 0.27538335, 0.25585952, 0.23805413, 0.2503961,
            0.25661182, 0.25243777, 0.22848365, 0.24415815, 0.24615972, 0.2710507, 0.21903253, 0.25936127, 0.25844643,
            0.23131096, 0.27529445, 0.22987027, 0.21125647, 0.25470695, 0.2497715, 0.2491689, 0.24892204, 0.20183493,
            0.25335476, 0.25675264, 0.2415058, 0.25321072, 0.23581976, 0.25297105, 0.23603949, 0.24867813, 0.2450876,
            0.22992566, 0.2515157, 0.252433, 0.22984442, 0.2551555, 0.22751407, 0.26367426, 0.25660837, 0.28338486,
            0.22689429, 0.22679853, 0.22788338, 0.23457928, 0.21873842, 0.22086981, 0.22314493, 0.22178757, 0.23675135,
            0.23140994, 0.20882371, 0.23465908, 0.2308408, 0.22733337, 0.2279475, 0.24072647, 0.18361235, 0.2328619,
            0.23607822, 0.21095647, 0.2289159, 0.26277477, 0.20239891, 0.2093859, 0.25178105, 0.21136248, 0.2573982,
            0.24435624, 0.21286884, 0.2121158, 0.27067178, 0.23667839, 0.24764377, 0.18809435, 0.27223253, 0.2243107,
            0.23626174, 0.21926042, 0.22744516, 0.2273939, 0.20839864, 0.24547954, 0.21811256, 0.23585406, 0.21638489,
            0.21470869, 0.2319687, 0.234081, 0.21581209, 0.20763728, 0.23624623, 0.2590341, 0.2683559, 0.20119594,
            0.22995698, 0.22340053, 0.23083866, 0.21803407, 0.22761846, 0.21162409, 0.24933328, 0.2225056, 0.20814365,
            0.2324424, 0.23053788, 0.243934, 0.23450236, 0.20846614, 0.21106933, 0.21672669, 0.23852359, 0.2780287,
            0.20528151, 0.22459666, 0.22605848, 0.19954711, 0.20797029, 0.20915599, 0.26152897, 0.23612721, 0.244094,
            0.22815058, 0.23181424, 0.2365928, 0.23985569, 0.20449898, 0.23513003, 0.21912873, 0.23280728, 0.21602273,
            0.21781024, 0.22914892, 0.25105777, 0.233763, 0.23805046, 0.19352783, 0.22324511, 0.24925143, 0.24771588,
            0.22568789, 0.18700835, 0.20701346, 0.23927462]
train_loss = [0.6915177, 0.5527179, 0.45415217, 0.4739744, 0.38712907, 0.43757302, 0.33305642, 0.37256116, 0.38898247,
              0.38376707, 0.37299752, 0.3433274, 0.3609689, 0.33166212, 0.31053174, 0.3415564, 0.32429403, 0.28016233,
              0.3635425, 0.32408988, 0.3143651, 0.315875, 0.3534422, 0.33692175, 0.27324295, 0.30141288, 0.36698866,
              0.36759356, 0.27479017, 0.29608363, 0.310682, 0.3149797, 0.29907304, 0.34043616, 0.28884232, 0.3919475,
              0.29441512, 0.30997604, 0.2830035, 0.32156676, 0.30848145, 0.22124547, 0.20020598, 0.27599555, 0.2662147,
              0.29233924, 0.25438365, 0.31769964, 0.24896619, 0.23927815, 0.25741476, 0.29988354, 0.22855268,
              0.24976406, 0.24144499, 0.28568515, 0.2581363, 0.27947754, 0.31297395, 0.22709112, 0.25211883, 0.2230346,
              0.3175438, 0.28275365, 0.26474112, 0.31951818, 0.28237617, 0.2751407, 0.2785843, 0.26603216, 0.27072716,
              0.2671839, 0.35600185, 0.275851, 0.23210043, 0.26232043, 0.288636, 0.25009483, 0.2639191, 0.24198967,
              0.2672183, 0.23899037, 0.30142397, 0.26198795, 0.2700096, 0.21330878, 0.24008316, 0.29315603, 0.28798294,
              0.26902422, 0.27099907, 0.25042343, 0.26926345, 0.28729576, 0.2721168, 0.2551556, 0.28675961, 0.24655262,
              0.21978135, 0.20067668, 0.28349757, 0.21237707, 0.223821, 0.275849, 0.231689, 0.28415364, 0.25976494,
              0.19928755, 0.19063114, 0.2579764, 0.26098144, 0.23412877, 0.26892838, 0.2371395, 0.2667834, 0.26184922,
              0.29118207, 0.20430669, 0.35364723, 0.19738278, 0.24251546, 0.25502247, 0.22134465, 0.22696061,
              0.24682048, 0.24877983, 0.25106594, 0.26034164, 0.26035488, 0.22997062, 0.27120602, 0.281497, 0.21070924,
              0.20868817, 0.24109149, 0.22396728, 0.25733495, 0.20764148, 0.19729257, 0.22388923, 0.27590075, 0.2559203,
              0.24188526, 0.26874384, 0.22505264, 0.2629392, 0.20739448, 0.23251846, 0.20680784, 0.21888316, 0.22544883,
              0.25524503, 0.22570705, 0.20370434, 0.2546929, 0.21571437, 0.26381996, 0.23818932, 0.20846725, 0.21887858,
              0.18658328, 0.23306641, 0.23837382, 0.20238614, 0.24019456, 0.2519886, 0.26583478, 0.27909094, 0.21717043,
              0.2400423, 0.2590289, 0.2459737, 0.2405231, 0.20786338, 0.2029769, 0.25950232, 0.24299821, 0.19523627,
              0.22559698, 0.26086202, 0.21439134, 0.2489351, 0.2201901, 0.2028038, 0.22406563, 0.2527458, 0.29478678,
              0.21247336, 0.25951743, 0.25689155, 0.23734644, 0.23314333, 0.2610395, 0.23569413, 0.25199318, 0.22062829,
              0.23091623, 0.20470658, 0.27669832, 0.18010688, 0.23328936, 0.26591516, 0.24165758, 0.219704, 0.22542964,
              0.2580334, 0.19968279, 0.20695838, 0.23448096, 0.23977159, 0.19490072, 0.21934527, 0.21380714, 0.24128836,
              0.24429807, 0.21609524, 0.26710337, 0.21708742, 0.21305035, 0.2170364, 0.1992341, 0.24691144, 0.2507223,
              0.2197228, 0.21660197, 0.2288482, 0.21934325, 0.22370772, 0.2618755, 0.20691934, 0.21334547, 0.21837479,
              0.23703182, 0.24723181, 0.22284728, 0.23311433, 0.27695143, 0.21597783, 0.21633554, 0.184131, 0.27550793,
              0.20637046, 0.24166793, 0.19442986, 0.2376487, 0.18930084, 0.2067484, 0.22109157, 0.2375186, 0.23804006,
              0.16391093, 0.21644238, 0.21582331, 0.19838825, 0.22190943, 0.20830625, 0.20220092, 0.2267437, 0.21428382,
              0.2633141, 0.2189481, 0.22108805, 0.24442218, 0.23193312, 0.20513633, 0.24193609, 0.24986489, 0.21965094,
              0.19188404, 0.26981607, 0.21115315, 0.24194288, 0.26427114, 0.18787968, 0.25074863, 0.2863277, 0.2604953,
              0.19155423, 0.21055616, 0.23453951, 0.16033213, 0.17663464, 0.18894172, 0.1852507, 0.25752455, 0.20373033,
              0.21635756, 0.2076844, 0.19321278, 0.21710305, 0.24515826, 0.2660146, 0.22600037, 0.21294606, 0.24185446,
              0.18887642, 0.20094377, 0.21383046, 0.26965675, 0.20693173, 0.1814681]
x = np.arange(len(train_loss))
plt.subplot(2, 2, 1)
plt.plot(x, val_loss)
plt.plot(x, train_loss)
plt.xlabel('step/50')
plt.ylabel('loss')
plt.ylim(0, 0.7)
plt.title("lr = 0.001 step=15000 batch=256 opt=sgd")
plt.legend(['val', 'train'])

# Final test accuracy = 96.8%, acc = 0.968379, prec = 0.971074, recall = 0.963115
# lr = 0.1 step=15000 batch=256 opt=adagrad
val_loss = [0.39933664, 0.42906857, 0.94833773, 1.4724548, 0.26719263, 0.53655624, 0.41254735, 0.2442723, 0.2352336,
            0.18661353, 0.21844345, 0.17374596, 0.14253835, 0.1536277, 0.17452146, 0.3045007, 0.18655078, 0.21893862,
            0.16899996, 0.20886503, 0.16197333, 0.24956287, 0.14999591, 0.24795234, 0.18041739, 0.1562467, 0.21274444,
            0.14529051, 0.20669913, 0.15685958, 0.16865811, 0.19277051, 0.19984236, 0.22571993, 0.27094868, 0.216773,
            0.18991223, 0.24602222, 0.14910176, 0.2165075, 0.16395718, 0.19560242, 0.1534469, 0.12773553, 0.16394542,
            0.1766325, 0.17754179, 0.1870021, 0.14046292, 0.1559634, 0.16401333, 0.12864433, 0.18504664, 0.12826303,
            0.1353778, 0.12092294, 0.14370957, 0.217092, 0.18511751, 0.15002266, 0.11264166, 0.1163497, 0.18558389,
            0.1545661, 0.13468364, 0.16215464, 0.12518078, 0.12580946, 0.20371862, 0.1709756, 0.13471623, 0.15647584,
            0.12892732, 0.13074824, 0.14836216, 0.14908955, 0.14643198, 0.17721508, 0.1826374, 0.14410038, 0.14538227,
            0.13571385, 0.113473296, 0.18484552, 0.1813043, 0.13062985, 0.15941855, 0.17902516, 0.18576619, 0.15723716,
            0.15192568, 0.11884425, 0.18251467, 0.11859731, 0.14686355, 0.13178577, 0.17177013, 0.1732965, 0.14026913,
            0.15164019, 0.1737941, 0.12970358, 0.13328677, 0.1529229, 0.1730296, 0.1481393, 0.12975289, 0.1543895,
            0.14546879, 0.16552731, 0.19296852, 0.13951927, 0.13818762, 0.16516648, 0.14840265, 0.13001545, 0.11946745,
            0.15812631, 0.14226118, 0.13364956, 0.13657105, 0.11900389, 0.13780192, 0.14803593, 0.1271064, 0.14342222,
            0.12791193, 0.12246914, 0.12786719, 0.12915473, 0.12191663, 0.15753852, 0.11058517, 0.1676798, 0.17901629,
            0.090464786, 0.159262, 0.12912953, 0.14058077, 0.15001564, 0.15561512, 0.11830531, 0.109959796, 0.13376,
            0.11970342, 0.18847641, 0.11997181, 0.15525615, 0.11920205, 0.14641479, 0.1399623, 0.11822969, 0.12169723,
            0.13341758, 0.16842367, 0.15341322, 0.11716423, 0.1869126, 0.15599099, 0.13174047, 0.17100844, 0.16387169,
            0.13848364, 0.15846846, 0.14976843, 0.13562477, 0.09641351, 0.10488936, 0.11166561, 0.14101824, 0.14293826,
            0.10623148, 0.13338047, 0.1040159, 0.12866397, 0.13440444, 0.122780025, 0.11790416, 0.12114663, 0.11715419,
            0.119385615, 0.12711775, 0.11756295, 0.12489493, 0.15021688, 0.116447985, 0.14797837, 0.14367159,
            0.11104744, 0.15709236, 0.112230025, 0.14664234, 0.120775074, 0.13239932, 0.15659733, 0.09797184,
            0.14192514, 0.15807687, 0.12398009, 0.11451334, 0.13874145, 0.15758753, 0.13557361, 0.13505992, 0.1443134,
            0.09756477, 0.108650684, 0.13953027, 0.17987743, 0.14085609, 0.10938068, 0.10557439, 0.14459464, 0.12886585,
            0.12530822, 0.1355695, 0.12618795, 0.11857875, 0.1224516, 0.120354176, 0.10808415, 0.09481001, 0.1175015,
            0.15086249, 0.14446416, 0.14868811, 0.16941166, 0.12788841, 0.12637001, 0.11712419, 0.12512378, 0.12973048,
            0.14369175, 0.12443836, 0.13467285, 0.13081464, 0.092247054, 0.14471954, 0.08472654, 0.13526666, 0.13084371,
            0.16020808, 0.13323174, 0.118508115, 0.1095069, 0.15181184, 0.14424197, 0.13395077, 0.08528595, 0.108952045,
            0.12317623, 0.11276773, 0.12237044, 0.12604728, 0.15960339, 0.12759016, 0.11496277, 0.13611597, 0.12286315,
            0.15059471, 0.13687229, 0.11369811, 0.110671, 0.09337784, 0.11666816, 0.11702745, 0.14038679, 0.13872805,
            0.1282534, 0.096620806, 0.12135327, 0.15069306, 0.1240598, 0.12057069, 0.08841292, 0.1260247, 0.09592661,
            0.108437605, 0.08690244, 0.20233862, 0.1042534, 0.17150477, 0.11619298, 0.121887095, 0.103364885,
            0.14988264, 0.10931728, 0.086780265, 0.14025235, 0.118109465, 0.12752488, 0.09107299, 0.124353155,
            0.12330603, 0.14832726, 0.13610584, 0.13478701, 0.10694432, 0.13237591, 0.12922892, 0.12520733]
train_loss = [0.6936997, 0.45227203, 0.9113723, 0.80229837, 0.35416096, 0.4914467, 0.6206801, 0.248385, 0.19125587,
              0.16703233, 0.22056429, 0.24536207, 0.23626569, 0.19948095, 0.19623035, 0.45860153, 0.19794412,
              0.21191236, 0.13662899, 0.23753425, 0.21880628, 0.22690085, 0.18158266, 0.21947953, 0.1680223, 0.14398798,
              0.1378518, 0.1391944, 0.1469906, 0.16463713, 0.18673919, 0.13899529, 0.15553553, 0.18921566, 0.19583634,
              0.14553148, 0.12827566, 0.18259624, 0.15694903, 0.13654627, 0.15719748, 0.17061237, 0.1662853, 0.11847578,
              0.14509594, 0.11717441, 0.14102152, 0.19666424, 0.12285184, 0.16116776, 0.11631102, 0.14727348, 0.1557469,
              0.14606124, 0.16455817, 0.1681544, 0.11349176, 0.20086592, 0.14663994, 0.16069725, 0.13992247, 0.16942804,
              0.14771378, 0.12164329, 0.17068867, 0.11604713, 0.13402852, 0.12730855, 0.16825172, 0.11517309,
              0.17124344, 0.1271091, 0.10392508, 0.11423378, 0.125895, 0.16365615, 0.13593543, 0.12642428, 0.14680576,
              0.09309753, 0.20680965, 0.09561458, 0.12976061, 0.13009787, 0.13721655, 0.13970765, 0.094521545,
              0.14846274, 0.14101347, 0.11584221, 0.088187344, 0.12454088, 0.15753144, 0.10953825, 0.14751711,
              0.115096584, 0.13291079, 0.1482283, 0.15064941, 0.09902228, 0.12656446, 0.15675078, 0.12794836,
              0.15043892, 0.12970133, 0.12371446, 0.1148302, 0.105699405, 0.12642005, 0.17295538, 0.11559475,
              0.13734609, 0.09096031, 0.12690067, 0.11932559, 0.10993421, 0.0924287, 0.12309119, 0.16553867, 0.14875206,
              0.10506258, 0.08170286, 0.1138199, 0.15301558, 0.11989636, 0.14424916, 0.111648135, 0.07777108,
              0.14135122, 0.111224815, 0.090653904, 0.0910226, 0.13288353, 0.12026472, 0.13740297, 0.10874984,
              0.10266722, 0.12706843, 0.098075874, 0.10897325, 0.13959044, 0.13893461, 0.10204399, 0.15411617,
              0.1415725, 0.109675676, 0.13145243, 0.13278192, 0.10527131, 0.14702387, 0.0926024, 0.11868636, 0.14733098,
              0.11960828, 0.086783685, 0.09126994, 0.112292916, 0.10456459, 0.077515885, 0.09830858, 0.18748254,
              0.10011226, 0.07268158, 0.13093072, 0.085816205, 0.12711592, 0.08499979, 0.13803917, 0.094664656,
              0.058019318, 0.118360415, 0.09867208, 0.111686856, 0.11115648, 0.07202667, 0.14486223, 0.104777806,
              0.14433855, 0.11253368, 0.11121832, 0.15278876, 0.09904848, 0.09559667, 0.09933751, 0.12901849,
              0.15159835, 0.1131963, 0.1561822, 0.14798316, 0.111732736, 0.08542847, 0.124013685, 0.10788925,
              0.08879903, 0.08412363, 0.09770229, 0.13084072, 0.12175507, 0.119601935, 0.1642844, 0.06316948,
              0.10497253, 0.10253346, 0.15283668, 0.14639094, 0.13634035, 0.09732254, 0.09479423, 0.11094244,
              0.087984055, 0.11278911, 0.10387624, 0.086909756, 0.06763905, 0.10209927, 0.11997986, 0.12339843,
              0.08666526, 0.10611755, 0.115738355, 0.11851434, 0.09597063, 0.08455622, 0.12081416, 0.11147103,
              0.10029048, 0.120842695, 0.096067056, 0.13608922, 0.13518053, 0.08236736, 0.11254154, 0.14435145,
              0.140077, 0.08964607, 0.108672135, 0.11671304, 0.1403617, 0.105117574, 0.11357903, 0.093339965,
              0.09589712, 0.12258699, 0.12898508, 0.12506312, 0.11403863, 0.07879685, 0.096218824, 0.10055617,
              0.09137784, 0.1078019, 0.095099464, 0.117360756, 0.10509485, 0.117460646, 0.10547118, 0.09215078,
              0.095423155, 0.16588612, 0.113565505, 0.11681831, 0.10095188, 0.112285316, 0.08198248, 0.13635671,
              0.11392206, 0.10807595, 0.15121448, 0.093659736, 0.08548164, 0.14000055, 0.09358464, 0.101327084,
              0.07813625, 0.11623634, 0.1201035, 0.119630106, 0.116944686, 0.10921849, 0.10213606, 0.087881885,
              0.17023143, 0.09388828, 0.13378806, 0.123365775, 0.06932058, 0.09841391, 0.09121656, 0.0937546,
              0.108981214, 0.13083221, 0.09859927, 0.09649566, 0.0944061, 0.06972307, 0.07752521, 0.0790948, 0.07632391,
              0.104396254, 0.090245776, 0.08389071]
x = np.arange(len(train_loss))
plt.subplot(2, 2, 2)
plt.plot(x, val_loss)
plt.plot(x, train_loss)
plt.xlabel('step/50')
plt.ylabel('loss')
plt.ylim(0, 0.7)
plt.title("lr = 0.1 step=15000 batch=256 opt=adagrad")
plt.legend(['val', 'train'])

# Final test accuracy = 94.8%, acc = 0.948027, prec = 0.934066, recall = 0.965909
val_loss = [1.8769636, 0.22412996, 0.19532058, 0.20000835, 0.15856378, 0.22581951, 0.19572864, 0.15914842, 0.18064557,
            0.17463589, 0.17717269, 0.19102937, 0.15442808, 0.30878615, 0.1392495, 0.18118125, 0.13217145, 0.14136963,
            0.20844476, 0.19019426, 0.17258975, 0.14279816, 0.13349041, 0.16133764, 0.11337802, 0.15273435, 0.14522718,
            0.13206837, 0.14056009, 0.17340975, 0.1442533, 0.14707223, 0.12575656, 0.15331703, 0.1614657, 0.17116882,
            0.12904541, 0.12667306, 0.13726172, 0.157116, 0.13095364, 0.16140011, 0.099213377, 0.11631969, 0.13324688,
            0.09737137, 0.17472589, 0.12781031, 0.18480761, 0.11506198, 0.14012225, 0.13949099, 0.13130854, 0.11530177,
            0.11395956, 0.11926414, 0.12087984, 0.12277774, 0.14594634, 0.11087441, 0.18443809, 0.16211978, 0.13877568,
            0.11448379, 0.127469, 0.13384457, 0.09099783, 0.14198446, 0.18007532, 0.14536716, 0.12552917, 0.19625708,
            0.12041479, 0.11766791, 0.17864966, 0.22537422, 0.19822924, 0.15084065, 0.15669337, 0.11831813, 0.14410636,
            0.097107813, 0.10357171, 0.14655177, 0.13102131, 0.19983175, 0.1832839, 0.098014638, 0.17354277, 0.11134109,
            0.14765373, 0.14360438, 0.10896276, 0.094510004, 0.17398444, 0.15720561, 0.08945775, 0.12452711, 0.15409152,
            0.1612846, 0.14288779, 0.11530221, 0.10337918, 0.14131606, 0.097785003, 0.1215905, 0.10695839, 0.15800753,
            0.11363676, 0.14864425, 0.16871278, 0.13408515, 0.11585532, 0.11561273, 0.089076929, 0.12049432, 0.15597336,
            0.17430812, 0.1702567, 0.12450618, 0.1937215, 0.19785091, 0.14996342, 0.17839022, 0.13741924, 0.1305214,
            0.14682774, 0.16852146, 0.11420649, 0.15634266, 0.1367979, 0.18337041, 0.13766903, 0.11548164, 0.16552433,
            0.12842688, 0.14263563, 0.1647841, 0.10733414, 0.14476693, 0.11598537, 0.14326738, 0.15595999, 0.277652,
            0.1296601, 0.12084368, 0.18226674, 0.17943507, 0.10813372, 0.21458617, 0.16371971, 0.074596331, 0.15833597,
            0.15031387, 0.14059521, 0.10853302, 0.14484999, 0.11612716, 0.15969482, 0.12021662, 0.16090037, 0.11240268,
            0.24013592, 0.10476056, 0.12431109, 0.13592418, 0.16529956, 0.11793328, 0.12599616, 0.10784318, 0.20350628,
            0.12187935, 0.14566714, 0.11226732, 0.1053707, 0.10665345, 0.079960905, 0.14086699, 0.16361672, 0.14169574,
            0.094001941, 0.19000329, 0.12490018, 0.10793475, 0.1417353, 0.099331804, 0.10528291, 0.19017825, 0.22141534,
            0.13006698, 0.11217178, 0.11348067, 0.15585089, 0.15841606, 0.14667386, 0.1686774, 0.12484099, 0.15920988,
            0.18231118, 0.11050647, 0.18733743, 0.278667, 0.16051792, 0.15921071, 0.15057346, 0.12217075, 0.1353775,
            0.13376883, 0.093743727, 0.10502876, 0.14943448, 0.13647573, 0.13256809, 0.10161494, 0.10805535, 0.11090929,
            0.14909974, 0.15248032, 0.075421795, 0.2686654, 0.17877688, 0.15678474, 0.17611611, 0.13991332, 0.10255666,
            0.13741666, 0.20447347, 0.11344709, 0.10939483, 0.106302, 0.14792347, 0.12750523, 0.14492413, 0.20465226,
            0.24418089, 0.12284365, 0.25745311, 0.14634097, 0.11705354, 0.12941562, 0.14978512, 0.13855599, 0.20939115,
            0.16546902, 0.15538393, 0.093817361, 0.10208764, 0.12296108, 0.088136278, 0.13100995, 0.11645264,
            0.13733567, 0.20312017, 0.13989127, 0.11388827, 0.18516219, 0.1380733, 0.141711, 0.110825, 0.15354215,
            0.069610506, 0.20160967, 0.18208891, 0.17225116, 0.11060813, 0.1540025, 0.10762737, 0.17940989, 0.10827032,
            0.10376641, 0.14070357, 0.11059956, 0.16339689, 0.15252471, 0.13375273, 0.17895709, 0.14008459, 0.13287245,
            0.17493182, 0.14807256, 0.1359698, 0.13297312, 0.12637034, 0.16342616, 0.25730687, 0.17962196, 0.1612,
            0.14902419, 0.12235332, 0.1629259, 0.16778024, 0.16230416, 0.11101843, 0.15020451, 0.19070525, 0.1617737,
            0.1588189, 0.088096835, 0.11117036, 0.13954808, 0.17239988]
train_loss = [0.69329166, 0.2241703, 0.1590614, 0.18699422, 0.23103769, 0.18281835, 0.18186782, 0.17681132, 0.11415073,
              0.16908324, 0.14961787, 0.17839377, 0.14707291, 0.22793844, 0.11684653, 0.13453273, 0.12264811,
              0.16277777, 0.13671318, 0.15212519, 0.13666916, 0.090343699, 0.12001908, 0.11761182, 0.066554286,
              0.12166078, 0.13408339, 0.038787853, 0.11974774, 0.11453336, 0.073481947, 0.13464671, 0.083329692,
              0.091136344, 0.14645404, 0.098266661, 0.11310287, 0.097987488, 0.13446864, 0.15320407, 0.14249936,
              0.094037302, 0.09882623, 0.084436916, 0.10077192, 0.087485507, 0.076752692, 0.092561185, 0.18510526,
              0.051655959, 0.13558596, 0.10067158, 0.1765883, 0.10883689, 0.11456522, 0.10056843, 0.10025963,
              0.088592909, 0.088932335, 0.13388467, 0.088844135, 0.087783977, 0.077108212, 0.087870277, 0.057743885,
              0.094277047, 0.13749674, 0.092220083, 0.070798092, 0.097450174, 0.2435074, 0.061579794, 0.092712738,
              0.10001256, 0.10701853, 0.04707586, 0.049243558, 0.097697385, 0.11634365, 0.09353698, 0.12774603,
              0.11031514, 0.058459029, 0.096067384, 0.09597335, 0.067252614, 0.13793294, 0.069386348, 0.106722,
              0.066164769, 0.10316232, 0.10736586, 0.14600055, 0.09225776, 0.061262846, 0.073974915, 0.043263659,
              0.098897845, 0.16013557, 0.076048143, 0.082178973, 0.13130781, 0.10355695, 0.12724665, 0.066595614,
              0.11306039, 0.11233602, 0.074413523, 0.074854501, 0.086141504, 0.1082591, 0.060495414, 0.063899904,
              0.12634945, 0.083638333, 0.098012522, 0.11536603, 0.13823397, 0.07738135, 0.099995606, 0.063759074,
              0.10488374, 0.078910425, 0.08955732, 0.070584014, 0.10546146, 0.060416915, 0.093167834, 0.06945578,
              0.078905024, 0.066838302, 0.043357339, 0.042201079, 0.10288718, 0.085320167, 0.092376888, 0.059309512,
              0.15266013, 0.18285802, 0.052607786, 0.078818515, 0.074193202, 0.066806406, 0.05221004, 0.11231259,
              0.090714566, 0.11285731, 0.1171367, 0.090709314, 0.12200575, 0.073911287, 0.09282615, 0.059587151,
              0.11256497, 0.081589572, 0.06126307, 0.071772233, 0.076485671, 0.11948161, 0.057108942, 0.074490622,
              0.10665395, 0.10801536, 0.059978139, 0.051235538, 0.064208433, 0.04779296, 0.061055455, 0.10031521,
              0.14399816, 0.079095587, 0.084608473, 0.12363049, 0.12211792, 0.070339426, 0.078241237, 0.068117484,
              0.079707339, 0.087132655, 0.046207309, 0.04715066, 0.18884492, 0.06585981, 0.048551105, 0.09518531,
              0.066377632, 0.27066296, 0.060618233, 0.10784589, 0.078355819, 0.045499966, 0.085403889, 0.046579689,
              0.074727468, 0.12215963, 0.044961344, 0.066112101, 0.086605221, 0.051376063, 0.078939676, 0.10931844,
              0.10470802, 0.096560389, 0.087651454, 0.068647981, 0.065011635, 0.10041159, 0.074916594, 0.025930813,
              0.043533318, 0.051970195, 0.028802279, 0.091628246, 0.077982493, 0.061223727, 0.061434358, 0.081506848,
              0.15168718, 0.091881491, 0.072177961, 0.04176423, 0.049105737, 0.036086086, 0.080677308, 0.1040054,
              0.08774969, 0.052374918, 0.22320662, 0.22843635, 0.11989394, 0.040965602, 0.056584097, 0.050082576,
              0.074463457, 0.033949401, 0.045717645, 0.048381101, 0.11110048, 0.04958583, 0.18950845, 0.066488259,
              0.033332542, 0.079616204, 0.084322788, 0.11208493, 0.12265364, 0.061106399, 0.039428115, 0.037283987,
              0.039621755, 0.063946486, 0.056581661, 0.073610783, 0.10487868, 0.075488508, 0.056491103, 0.079794243,
              0.064103521, 0.05247556, 0.066131242, 0.15704125, 0.05420991, 0.058098059, 0.098936163, 0.082323864,
              0.071779616, 0.066443145, 0.12648988, 0.04879614, 0.091144487, 0.095825665, 0.087471426, 0.083401412,
              0.13588361, 0.081712916, 0.078658819, 0.10978767, 0.077381566, 0.033788808, 0.059670612, 0.068236917,
              0.13463749, 0.043550164, 0.064226806, 0.072317861, 0.030915802, 0.05509223, 0.037256785, 0.082997181,
              0.057547759, 0.048778474, 0.093014188, 0.077620246, 0.054421261, 0.084764265, 0.035846762, 0.11309559,
              0.075257815, 0.06554845, 0.10701479, 0.043122821]
x = np.arange(len(train_loss))
plt.subplot(2, 2, 3)
plt.plot(x, val_loss)
plt.plot(x, train_loss)
plt.xlabel('step/50')
plt.ylabel('loss')
plt.ylim(0, 0.7)
plt.title("lr = 0.01 step=15000 batch=256 opt=adamgrad")
plt.legend(['val', 'train'])
plt.show()


# Step 13600: Val acc on random sampled 512 examples = 91.2%, val loss: 0.2261, train loss: 0.2643
# Step 13650: Val acc on random sampled 512 examples = 92.6%, val loss: 0.1995, train loss: 0.1879
# Step 13700: Val acc on random sampled 512 examples = 91.4%, val loss: 0.2080, train loss: 0.2507
# Step 13750: Val acc on random sampled 512 examples = 92.0%, val loss: 0.2092, train loss: 0.2863
# Step 13800: Val acc on random sampled 512 examples = 87.9%, val loss: 0.2615, train loss: 0.2605
# Step 13850: Val acc on random sampled 512 examples = 89.8%, val loss: 0.2361, train loss: 0.1916
# Step 13900: Val acc on random sampled 512 examples = 90.8%, val loss: 0.2441, train loss: 0.2106
# Step 13950: Val acc on random sampled 512 examples = 90.6%, val loss: 0.2282, train loss: 0.2345
# Step 14000: Val acc on random sampled 512 examples = 90.4%, val loss: 0.2318, train loss: 0.1603
# Step 14050: Val acc on random sampled 512 examples = 89.6%, val loss: 0.2366, train loss: 0.1766
# Step 14100: Val acc on random sampled 512 examples = 88.9%, val loss: 0.2399, train loss: 0.1889
# Step 14150: Val acc on random sampled 512 examples = 91.4%, val loss: 0.2045, train loss: 0.1853
# Step 14200: Val acc on random sampled 512 examples = 89.6%, val loss: 0.2351, train loss: 0.2575
# Step 14250: Val acc on random sampled 512 examples = 91.6%, val loss: 0.2191, train loss: 0.2037
# Step 14300: Val acc on random sampled 512 examples = 90.8%, val loss: 0.2328, train loss: 0.2164
# Step 14350: Val acc on random sampled 512 examples = 91.2%, val loss: 0.2160, train loss: 0.2077
# Step 14400: Val acc on random sampled 512 examples = 90.2%, val loss: 0.2178, train loss: 0.1932
# Step 14450: Val acc on random sampled 512 examples = 89.5%, val loss: 0.2291, train loss: 0.2171
# Step 14500: Val acc on random sampled 512 examples = 88.9%, val loss: 0.2511, train loss: 0.2452
# Step 14550: Val acc on random sampled 512 examples = 89.8%, val loss: 0.2338, train loss: 0.2660
# Step 14600: Val acc on random sampled 512 examples = 91.4%, val loss: 0.2381, train loss: 0.2260
# Step 14650: Val acc on random sampled 512 examples = 92.8%, val loss: 0.1935, train loss: 0.2129
# Step 14700: Val acc on random sampled 512 examples = 90.8%, val loss: 0.2232, train loss: 0.2419
# Step 14750: Val acc on random sampled 512 examples = 89.8%, val loss: 0.2493, train loss: 0.1889
# Step 14800: Val acc on random sampled 512 examples = 88.9%, val loss: 0.2477, train loss: 0.2009
# Step 14850: Val acc on random sampled 512 examples = 93.4%, val loss: 0.2257, train loss: 0.2138
# Step 14900: Val acc on random sampled 512 examples = 93.6%, val loss: 0.1870, train loss: 0.2697
# Step 14950: Val acc on random sampled 512 examples = 91.0%, val loss: 0.2070, train loss: 0.2069
# Step 14999: Val acc on random sampled 512 examples = 90.0%, val loss: 0.2393, train loss: 0.1815


# Step 14050: Val acc on random sampled 512 examples = 92.6%, val loss: 0.1715, train loss: 0.1702
# Step 14100: Val acc on random sampled 512 examples = 94.5%, val loss: 0.1162, train loss: 0.0939
# Step 14150: Val acc on random sampled 512 examples = 95.1%, val loss: 0.1219, train loss: 0.1338
# Step 14200: Val acc on random sampled 512 examples = 96.5%, val loss: 0.1034, train loss: 0.1234
# Step 14250: Val acc on random sampled 512 examples = 94.5%, val loss: 0.1499, train loss: 0.0693
# Step 14300: Val acc on random sampled 512 examples = 96.1%, val loss: 0.1093, train loss: 0.0984
# Step 14350: Val acc on random sampled 512 examples = 97.1%, val loss: 0.0868, train loss: 0.0912
# Step 14400: Val acc on random sampled 512 examples = 95.5%, val loss: 0.1403, train loss: 0.0938
# Step 14450: Val acc on random sampled 512 examples = 96.3%, val loss: 0.1181, train loss: 0.1090
# Step 14500: Val acc on random sampled 512 examples = 95.3%, val loss: 0.1275, train loss: 0.1308
# Step 14550: Val acc on random sampled 512 examples = 96.5%, val loss: 0.0911, train loss: 0.0986
# Step 14600: Val acc on random sampled 512 examples = 94.7%, val loss: 0.1244, train loss: 0.0965
# Step 14650: Val acc on random sampled 512 examples = 95.3%, val loss: 0.1233, train loss: 0.0944
# Step 14700: Val acc on random sampled 512 examples = 95.7%, val loss: 0.1483, train loss: 0.0697
# Step 14750: Val acc on random sampled 512 examples = 95.5%, val loss: 0.1361, train loss: 0.0775
# Step 14800: Val acc on random sampled 512 examples = 95.1%, val loss: 0.1348, train loss: 0.0791
# Step 14850: Val acc on random sampled 512 examples = 96.3%, val loss: 0.1069, train loss: 0.0763
# Step 14900: Val acc on random sampled 512 examples = 96.3%, val loss: 0.1324, train loss: 0.1044
# Step 14950: Val acc on random sampled 512 examples = 96.5%, val loss: 0.1292, train loss: 0.0902
# Step 14999: Val acc on random sampled 512 examples = 95.5%, val loss: 0.1252, train loss: 0.0839

# Step 13050: Val acc on random sampled 512 examples = 92.8%, val loss: 0.2016, train loss: 0.0542
# Step 13100: Val acc on random sampled 512 examples = 95.5%, val loss: 0.1821, train loss: 0.0581
# Step 13150: Val acc on random sampled 512 examples = 93.4%, val loss: 0.1723, train loss: 0.0989
# Step 13200: Val acc on random sampled 512 examples = 95.3%, val loss: 0.1106, train loss: 0.0823
# Step 13250: Val acc on random sampled 512 examples = 95.9%, val loss: 0.1540, train loss: 0.0718
# Step 13300: Val acc on random sampled 512 examples = 96.5%, val loss: 0.1076, train loss: 0.0664
# Step 13350: Val acc on random sampled 512 examples = 94.1%, val loss: 0.1794, train loss: 0.1265
# Step 13400: Val acc on random sampled 512 examples = 95.5%, val loss: 0.1083, train loss: 0.0488
# Step 13450: Val acc on random sampled 512 examples = 95.9%, val loss: 0.1038, train loss: 0.0911
# Step 13500: Val acc on random sampled 512 examples = 95.5%, val loss: 0.1407, train loss: 0.0958
# Step 13550: Val acc on random sampled 512 examples = 96.5%, val loss: 0.1106, train loss: 0.0875
# Step 13600: Val acc on random sampled 512 examples = 93.8%, val loss: 0.1634, train loss: 0.0834
# Step 13650: Val acc on random sampled 512 examples = 94.9%, val loss: 0.1525, train loss: 0.1359
# Step 13700: Val acc on random sampled 512 examples = 96.1%, val loss: 0.1338, train loss: 0.0817
# Step 13750: Val acc on random sampled 512 examples = 95.9%, val loss: 0.1790, train loss: 0.0787
# Step 13800: Val acc on random sampled 512 examples = 96.1%, val loss: 0.1401, train loss: 0.1098
# Step 13850: Val acc on random sampled 512 examples = 96.5%, val loss: 0.1329, train loss: 0.0774
# Step 13900: Val acc on random sampled 512 examples = 94.9%, val loss: 0.1749, train loss: 0.0338
# Step 13950: Val acc on random sampled 512 examples = 95.7%, val loss: 0.1481, train loss: 0.0597
# Step 14000: Val acc on random sampled 512 examples = 94.5%, val loss: 0.1360, train loss: 0.0682
# Step 14050: Val acc on random sampled 512 examples = 96.1%, val loss: 0.1330, train loss: 0.1346
# Step 14100: Val acc on random sampled 512 examples = 94.1%, val loss: 0.1264, train loss: 0.0436
# Step 14150: Val acc on random sampled 512 examples = 95.1%, val loss: 0.1634, train loss: 0.0642
# Step 14200: Val acc on random sampled 512 examples = 94.1%, val loss: 0.2573, train loss: 0.0723
# Step 14250: Val acc on random sampled 512 examples = 94.1%, val loss: 0.1796, train loss: 0.0309
# Step 14300: Val acc on random sampled 512 examples = 95.3%, val loss: 0.1612, train loss: 0.0551
# Step 14350: Val acc on random sampled 512 examples = 94.9%, val loss: 0.1490, train loss: 0.0373
# Step 14400: Val acc on random sampled 512 examples = 95.7%, val loss: 0.1224, train loss: 0.0830
# Step 14450: Val acc on random sampled 512 examples = 95.3%, val loss: 0.1629, train loss: 0.0575
# Step 14500: Val acc on random sampled 512 examples = 94.9%, val loss: 0.1678, train loss: 0.0488
# Step 14550: Val acc on random sampled 512 examples = 94.1%, val loss: 0.1623, train loss: 0.0930
# Step 14600: Val acc on random sampled 512 examples = 95.7%, val loss: 0.1110, train loss: 0.0776
# Step 14650: Val acc on random sampled 512 examples = 94.5%, val loss: 0.1502, train loss: 0.0544
# Step 14700: Val acc on random sampled 512 examples = 94.5%, val loss: 0.1907, train loss: 0.0848
# Step 14750: Val acc on random sampled 512 examples = 94.5%, val loss: 0.1618, train loss: 0.0358
# Step 14800: Val acc on random sampled 512 examples = 94.5%, val loss: 0.1588, train loss: 0.1131
# Step 14850: Val acc on random sampled 512 examples = 96.1%, val loss: 0.0881, train loss: 0.0753
# Step 14900: Val acc on random sampled 512 examples = 96.1%, val loss: 0.1112, train loss: 0.0655
# Step 14950: Val acc on random sampled 512 examples = 94.9%, val loss: 0.1395, train loss: 0.1070
# Step 14999: Val acc on random sampled 512 examples = 94.7%, val loss: 0.1724, train loss: 0.0431

